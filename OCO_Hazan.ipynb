{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1\n",
    "\n",
    "1. Where is the metric of succes in online learning borrowed from?\n",
    "\n",
    "- Game Theory\n",
    "\n",
    "2. The framework is closely tied to ?\n",
    "\n",
    "- Statistical Learning Theory\n",
    "\n",
    "- Convex Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Why should the losses determined by the adversary be not allowed to be unbounded?\n",
    "\n",
    "- The advesary can make the losses in the intitial rounds really huge and the remainging rounds reduce the scale.\n",
    "\n",
    "- This implies the adversary can never recover from the initial losses.\n",
    "\n",
    "4. What is required of the decision set? What is the problem of making decisions with an infinite set of decisions with no structure?\n",
    "\n",
    "- The decision set must be somehow bounded and /or structured though not necessarily finite\n",
    "\n",
    "- With an infinite set with no structure an advesary can assign high losses to the actions chosen by an adversary indefinitely, while setting apart some strategies  with zero loss. This precludes any meaningful performance metric.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. State the OCO framework.\n",
    "\n",
    "- Decision space =  $K$\n",
    "\n",
    "- Losses are convex functions over $K$\n",
    "\n",
    "- At time $t$ the user plays an $x\\in K$\n",
    "\n",
    "- once the user commits to a decision, it receieves loss determined by $f_t(x_t)$, where $f_t \\in F$ and $F$ is a family of convex loss functions over $K$\n",
    "\n",
    "- It is interesting to note that the user might see the entire $f_t$ or just $f_t(x_t)$ and this gives rise to different cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Formally define the regret after $T$ iterations.\n",
    "\n",
    "- $\\text{Regret}_T(A) = \\sup_{f_1, \\ldots, f_T \\in F}\\{\\sum_{i\\in [k]} f_t(x_t) - \\min_{x\\in K}\\sum_{t\\in [T]} f_t(x) \\}$\n",
    "\n",
    "- Notice that $f_1, \\ldots, f_t$ are chosen adverserially\n",
    "\n",
    "7. Intuitively, when does an algorithm do well?\n",
    "\n",
    "- If it achieves sublinear regret as function of $T$ because then on average it performs as well as the best fixed strategy in hindsight.\n",
    "\n",
    "8. Read examples: Prediction from Expert advice, Online Spam Filtering, Online Shortest Paths, Portfolio Selection, Matrix Completion and recommendation systems.\n",
    "\n",
    "9. What is scaled square loss?\n",
    "\n",
    "- $\\ell(\\hat{b}, b) = \\frac{1}{4}(\\hat{b} - b)^2$\n",
    "\n",
    "- why use a sqaure loss rather than any other function? The most natural choice beings perhaps one when $\\hat{b} = b$ and $0$ otherwise.\n",
    "    - First notice that if $\\hat{b}, b \\in \\{-1,1\\}$ then the square loss is indeed $0$ or $1$.\n",
    "    - Moving to a continuous space gives more flexibility, for example $\\hat{b}$ can now be in $[-1,1]$ depending on the confidence over the returned value\n",
    "    - Another reason is the algorithmic efficiency in finding a good solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. What is the loss of a decision maker choosing actions at random?\n",
    "\n",
    "- Trivially attains a loss of $T/2$\n",
    "\n",
    "- In terms of number of mistakes no algorithm can do better. We will devise a randomised setting in which the expected number of mistakes of any algorithm is at least $\\frac{T}{2}$\n",
    "\n",
    "11. State the Theorem that shows the relative performance of any deterministic algorithm compared to the best expert in hindsight is bad.\n",
    "\n",
    "- Let $L \\leq \\frac{T}{2}$ denote the number of mistakes made by the best expert in hindsight. Then there does not exist a deterministic algorithm that can guarantee than less than $2L$ mistakes.\n",
    "\n",
    "- The above theorem motivates the design of randomizes decision making in algorithms, and indeed OCO framework gracefully models decisions on a continutous probability space.\n",
    "\n",
    "\n",
    "12. State the theorem that gives an algorithm with performance guarantees compared to the best expert.\n",
    "\n",
    "- Let $\\epsilon \\in (0,\\frac{1}{2})$. Then there is an efficient deterministic algorithm that can guarantee less than $2(1+\\epsilon)L + \\frac{2\\log N}{\\epsilon}$ mistakes. The algorithm is __Weighted Majority__.\n",
    "\n",
    "- Let $\\epsilon$ be as above. Then there is an efficient randomized algorithm that can guarantee less than $L(1 + \\epsilon) + \\frac{\\log N}{\\epsilon}$ mistakes The algorithm is __Randomized Weighted Majority__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. State the Weighted Majority Algorithm\n",
    "\n",
    "14. State the Randomized Weighted Majority Algorithm"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
